{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import functools\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List,Annotated, Optional,Dict,TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_core.messages import HumanMessage,BaseMessage,SystemMessage,ToolMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph.message import add_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY=os.environ[\"GOOGLE_API_KEY\"]\n",
    "persist_directory = 'docs/chroma/'\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    max_tokens=250,\n",
    "    temperature=0.5)\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "vectorstore = Chroma(persist_directory=persist_directory,\n",
    "                              embedding_function=embeddings,\n",
    "                              collection_name = \"guide_questions\")\n",
    "\n",
    "question_retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "Failure\n",
      "scenarios\n",
      "for\n",
      "the\n",
      "systems\n",
      "1.\n",
      "Polar\n",
      "●\n",
      "Failure\n",
      "case:-\n",
      "Direct\n",
      "power\n",
      "source\n",
      "is\n",
      "power\n",
      "off\n",
      "or\n",
      "removed \n",
      "○\n",
      "Question\n",
      ":-\n",
      "Is\n",
      "the\n",
      "device's\n",
      "adapter\n",
      "AC\n",
      "current\n",
      "connected? \n",
      "●\n",
      "Failure\n",
      "case:-\n",
      "Temparature\n",
      "sensor\n",
      "is\n",
      "malfunctioning \n",
      "○\n",
      "Question\n",
      ":-\n",
      "Is\n",
      "there\n",
      "a\n",
      "potal\n",
      "solwing\n",
      "sensor\n",
      "error? \n",
      "○\n",
      "Question\n",
      ":-\n",
      "Is\n",
      "sensor\n",
      "wire\n",
      "damaged? \n",
      "2.\n",
      "Common\n",
      "●\n",
      "Failure\n",
      "case:-\n",
      "SIM\n",
      "damaged \n",
      "○\n",
      "Question\n",
      ":-\n",
      "Is\n",
      "water\n",
      "present\n",
      "inside\n",
      "the\n",
      "sensor/are\n",
      "there\n",
      "rust\n",
      "inside\n",
      "the \n",
      "sensor\n",
      "circuitry \n",
      "●\n",
      "Failure\n",
      "case:-\n",
      "GSM\n",
      "module\n",
      "damaged \n",
      "○\n",
      "Question\n",
      ":-\n",
      "Is\n",
      "there\n",
      "an\n",
      "LED\n",
      "blinking\n",
      "within\n",
      "the\n",
      "device? \n",
      "○\n",
      "Question\n",
      ":-\n",
      "Is\n",
      "there\n",
      "water\n",
      "inside\n",
      "the\n",
      "device?\n",
      "○\n",
      "Question\n",
      ":-\n",
      "Is\n",
      "sensor\n",
      "wire\n",
      "damaged? \n",
      "●\n",
      "Failure\n",
      "Case:-\n",
      "GSM\n",
      "module\n",
      "loss\n",
      "connection\n",
      "with\n",
      "main\n",
      "board \n",
      "○\n",
      "Question\n",
      ":-\n",
      "Is\n",
      "the\n",
      "board's\n",
      "LED\n",
      "working? \n",
      "○\n",
      "Question\n",
      ":-\n",
      "Is\n",
      "the\n",
      "device's\n",
      "GSM\n",
      "LED\n",
      "off?\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = question_retriever.invoke(\"Polar issues\",k=1)\n",
    "retrieved_docs_str = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "print(retrieved_docs_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"pdf_question_vdb_tool\")\n",
    "def pdf_question_vdb_tool(query:str) -> str:\n",
    "  \"\"\"\n",
    "  Search for questions to be asked regarding failures of devices for the provided user query.\n",
    "\n",
    "  Args:\n",
    "      query (str): user query which can be the question or any request related to the device failure details.\n",
    "\n",
    "  Returns:\n",
    "      str: retrieved relevant question from the documents for the user query which is formatted into string datatype\n",
    "  \"\"\"\n",
    "  retrieved_docs = question_retriever.invoke(query)\n",
    "\n",
    "  retrieved_docs_str = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "  return retrieved_docs_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages:Annotated[list,add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [pdf_question_vdb_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,tools, model, prompt):\n",
    "        self.prompt = prompt\n",
    "        self.model = model\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"chatbot\", self.call_llm)\n",
    "        graph.add_node(\"tools\", self.tool_action)\n",
    "        graph.set_entry_point(\"chatbot\")\n",
    "        #graph.add_edge(\"chatbot\", END)\n",
    "        graph.add_edge(\"tools\", \"chatbot\")\n",
    "        graph.add_conditional_edges(\n",
    "            \"chatbot\",\n",
    "            self.exists_action,\n",
    "            {True: \"tools\",False:END}\n",
    "        )\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "\n",
    "    def call_llm(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        #if self.system:\n",
    "        #    messages = [SystemMessage(content=self.system)] + messages\n",
    "        self.chain = self.prompt|self.llm\n",
    "        message = self.chain.invoke(\n",
    "            messages)\n",
    "        return {'messages': [message]}\n",
    "    \n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "    \n",
    "    def tool_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t['name'] in self.tools:      # check for bad tool name from LLM\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "            else:\n",
    "                result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an intelligent questioning assistant.\"\n",
    "        \"Based on the query you must decide the system and questions you should ask from the user\"\n",
    "        \"You must retrieve the necessary question and question from the user\"\n",
    "        \"Ask the suitable question from the user with the provided information\"\n",
    "    ),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "    ])\n",
    "    \n",
    "\n",
    "abot = Agent(tools,llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(abot.graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Agent' object has no attribute 'llm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi. my polar system isnt working. what could be the issue?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m abot\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages})\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1345\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1345\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1347\u001b[0m     config,\n\u001b[1;32m   1348\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1349\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1350\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1351\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1352\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1354\u001b[0m ):\n\u001b[1;32m   1355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1356\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1029\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m-> 1029\u001b[0m _panic_or_proceed(done, inflight, loop\u001b[38;5;241m.\u001b[39mstep)\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1456\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step, timeout_exc_cls)\u001b[0m\n\u001b[1;32m   1454\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1455\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1461\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/langgraph/pregel/executor.py:60\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m         task\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/langgraph/pregel/retry.py:25\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     23\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, task\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/langchain_core/runnables/base.py:2876\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2874\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   2875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2876\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2877\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2878\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/langgraph/utils.py:102\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m    101\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 102\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[33], line 25\u001b[0m, in \u001b[0;36mAgent.call_llm\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     22\u001b[0m messages \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#if self.system:\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#    messages = [SystemMessage(content=self.system)] + messages\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m|\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\n\u001b[1;32m     26\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39minvoke(messages)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m: [message]}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'llm'"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Hi. my polar system isnt working. what could be the issue?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi. my polar device isnt working. any issues?', id='42f2c1fc-6edb-4695-8a51-0e7a4f3821b2'),\n",
       " AIMessage(content='Is your Polar device a watch, a heart rate monitor, or something else? \\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-b3ceb78a-ce83-4802-bd30-162254bba069-0', usage_metadata={'input_tokens': 176, 'output_tokens': 16, 'total_tokens': 192})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi. my polar device isnt working. any issues?', id='2efbe52d-9426-4c92-bad1-014535355998'),\n",
       "  AIMessage(content='I can help you with that.  What kind of Polar device is it? \\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-6f9b8968-24b4-42e7-8e20-9d786eb86c41-0', usage_metadata={'input_tokens': 180, 'output_tokens': 16, 'total_tokens': 196})]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
